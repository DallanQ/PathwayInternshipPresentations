{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9894eef3-c141-4b25-8483-80456a716589",
   "metadata": {},
   "source": [
    "# How to crawl and extract data from Web pages\n",
    "\n",
    "This notebook is broken into three sections:\n",
    "- Crawling traditional websites using requests\n",
    "- Crawling javascript websites using playwright\n",
    "- Extracting data from web pages using BeautifulSoup\n",
    "\n",
    "Regardless of how you crawl a website, please remember three things:\n",
    "- Obey the robot exclusion protocol rules found in the robots.txt file at the root of nearly every domain: https://domain-name/robots.txt\n",
    "- Put (at least) a 3 second delay between requests.\n",
    "- Respect copyrights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "592b7f0e-e573-4a34-b4ac-297f8cbaf434",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2843715-cebf-4724-acaf-bce25399ff5a",
   "metadata": {},
   "source": [
    "## Crawling traditional websites using requests\n",
    "\n",
    "The python requests library makes http(s) requests to fetch web pages just like your browser.\n",
    "\n",
    "We'll start at a \"root\" page and use BeautifulSoup to find links to additional pages to crawl. We'll learn more about BeautifulSoup later.\n",
    "\n",
    "Let's use requests to crawl general conference talks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T06:51:20.014116631Z",
     "start_time": "2024-06-02T06:51:20.013126125Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "from typing import Optional\n",
    "from typing import Tuple\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03ac03aa-76a7-433a-843e-64412f31f787",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2024\n",
    "month = '04'\n",
    "host = 'https://www.churchofjesuschrist.org'\n",
    "base_dir = 'data/raw'\n",
    "bs_parser = 'html.parser'\n",
    "delay_seconds = 5\n",
    "\n",
    "if not os.path.exists(base_dir):\n",
    "    os.makedirs(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2247b452-8ac7-4386-81a3-4ad24d6f53be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _is_talk_url(url):\n",
    "    \"\"\"A talk URL has 6 components (first component is empty) and last component does not end in -session.\"\"\"\n",
    "    path_components = urlparse(url).path.split('/')\n",
    "    return len(path_components) == 6 and not path_components[-1].endswith('-session')\n",
    "\n",
    "\n",
    "def get_talk_urls(base_url, html):\n",
    "    \"\"\"Find all talk URLs on the page.\"\"\"\n",
    "    soup = BeautifulSoup(html, bs_parser)\n",
    "    return [urljoin(base_url, a['href']) for a in soup.find_all('a', href=True) \\\n",
    "            if _is_talk_url(urljoin(base_url, a['href']))]\n",
    "\n",
    "\n",
    "def get_talk_path(url):\n",
    "    \"\"\"Return the file path for saving the talk.\"\"\"\n",
    "    path_components = urlparse(url).path.split('/')\n",
    "    year, month, title = path_components[3:6]\n",
    "    return os.path.join(base_dir, f\"{year}-{month}-{title}.json\")\n",
    "\n",
    "# this function uses type hints like typescript to help your IDE detect errors in what you pass to the function\n",
    "def get_page(\n",
    "    url: str,\n",
    "    delay_seconds: int = 30,\n",
    "    headers: Optional[dict[str, str]] = None,\n",
    "    encoding: str = \"utf-8\",\n",
    "    timeout: int = 30,\n",
    ") -> Tuple[int, str]:\n",
    "    \"\"\"Get page from url.\"\"\"\n",
    "    if headers is None:\n",
    "        # make your program look like a chrome\n",
    "        headers = {\n",
    "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\",  # noqa: B950\n",
    "            \"Accept-Encoding\": \"gzip, deflate\",  # gzip, deflate, br, zstd\n",
    "            \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "            \"Cache-Control\": \"no-cache\",\n",
    "            \"Cookie\": \"TAsessionID=8f51490c-5611-45b6-9847-8585037a0e1b|NEW; notice_behavior=implied|us; gpv_Page=general-conference%7C2024%7C04%7C11oaks; gpv_cURL=www.churchofjesuschrist.org%2Fstudy%2Fgeneral-conference%2F2024%2F04%2F11oaks; s_ips=838; s_tp=1517; s_ppv=general-conference%257C2024%257C04%257C11oaks%2C55%2C55%2C55%2C838%2C1%2C1; AMCVS_66C5485451E56AAE0A490D45%40AdobeOrg=1; AMCV_66C5485451E56AAE0A490D45%40AdobeOrg=179643557%7CMCIDTS%7C19909%7CMCMID%7C88116570082250571280802679967931299750%7CMCAAMLH-1720711596%7C9%7CMCAAMB-1720711596%7C6G1ynYcLPuiQxYZrsz_pkqfLG9yMXBpb2zX5dvJdYQJzPXImdj0y%7CMCOPTOUT-1720113996s%7CNONE%7CvVersion%7C5.5.0; s_cc=true; s_plt=1.01; s_pltp=general-conference%7C2024%7C04%7C11oaks; adcloud={%22_les_v%22:%22c%2Cy%2Cchurchofjesuschrist.org%2C1720108596%22}; at_check=true; mbox=session#6bb5efff4aea494c8e2e9c7d3469ab29#1720108658|PC#6bb5efff4aea494c8e2e9c7d3469ab29.35_0#1783351598\",\n",
    "            \"Pragma\": \"no-cache\",\n",
    "            \"Priority\": \"u=0, i\",\n",
    "            \"Sec-Ch-Ua\": '\"Google Chrome\";v=\"125\", \"Chromium\";v=\"125\", \"Not.A/Brand\";v=\"24\"',\n",
    "            \"Sec-Ch-Ua-Mobile\": \"?0\",\n",
    "            \"Sec-Ch-Ua-Platform\": '\"macOS\"',\n",
    "            \"Sec-Fetch-Dest\": \"document\",\n",
    "            \"Sec-Fetch-Mode\": \"navigate\",\n",
    "            \"Sec-Fetch-Site\": \"same-origin\",\n",
    "            \"Sec-Fetch-User\": \"?1\",\n",
    "            \"Upgrade-Insecure-Requests\": \"1\",\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36\",  # noqa: B950\n",
    "        }\n",
    "    # make the request\n",
    "    response = requests.get(url, headers=headers, timeout=timeout)\n",
    "    # wait \n",
    "    time.sleep(delay_seconds)\n",
    "    if encoding:\n",
    "        response.encoding = encoding\n",
    "    return response.status_code, response.text\n",
    "\n",
    "\n",
    "def save_page(path: str, url: str, html: str, encoding: str = \"utf-8\") -> None:\n",
    "    \"\"\"Save page url and html to path.\"\"\"\n",
    "    page_info = {\n",
    "        \"url\": url,\n",
    "        \"html\": html,\n",
    "    }\n",
    "    with open(path, \"w\", encoding=encoding) as f:\n",
    "        json.dump(page_info, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a834cc6-b38e-4a40-9511-d14e34addec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_url = f\"{host}/study/general-conference/{year}/{month}?lang=eng\"\n",
    "# get the root page\n",
    "status_code, dir_html = get_page(dir_url, delay_seconds)\n",
    "if status_code != 200:\n",
    "    print(f\"Status code={status_code} url={dir_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93f5ce32-b09e-491b-a684-3af32f76a62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.churchofjesuschrist.org/study/general-conference/2024/04?lang=eng 68\n"
     ]
    }
   ],
   "source": [
    "# get all of the talk URLs from the conference root\n",
    "talk_urls = get_talk_urls(dir_url, dir_html)\n",
    "print(dir_url, len(talk_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1193cc33-661f-412b-ac61-f6400c84bd04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     data/raw/2024-04-11oaks.json\n",
      "     data/raw/2024-04-12larson.json\n",
      "     data/raw/2024-04-13holland.json\n",
      "     data/raw/2024-04-14dennis.json\n",
      "     data/raw/2024-04-15dushku.json\n",
      "     data/raw/2024-04-16soares.json\n",
      "     data/raw/2024-04-17gerard.json\n",
      "     data/raw/2024-04-18eyring.json\n",
      "     data/raw/2024-04-21bednar.json\n",
      "     data/raw/2024-04-22de-feo.json\n",
      "     data/raw/2024-04-23nielson.json\n",
      "     data/raw/2024-04-24alonso.json\n"
     ]
    }
   ],
   "source": [
    "# fetch each talk\n",
    "for ix, talk_url in enumerate(talk_urls):\n",
    "    path = get_talk_path(talk_url)\n",
    "    # don't re-crawl if you've already crawled\n",
    "    if os.path.exists(path):\n",
    "        continue\n",
    "    print(\"    \", path)\n",
    "    status_code, talk_html = get_page(talk_url, delay_seconds)\n",
    "    if status_code != 200:\n",
    "        print(f\"Status code={status_code} url={talk_url}\")\n",
    "        continue\n",
    "    save_page(path, talk_url, talk_html)\n",
    "    if ix > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5c1faf-0eb3-4430-9fb1-e8068a9ae75e",
   "metadata": {},
   "source": [
    "## Crawling javascript websites using playwright\n",
    "\n",
    "For some websites, the web page is simply a skeleton HTML plus some javascript. The browser has to execute the javascript to populate the full HTML.\n",
    "\n",
    "The Playwright library lets you control a browser from your python program: https://playwright.dev/python/docs/library\n",
    "\n",
    "Playwright has it's own way to find links to additional pages to crawl that uses XPaths, so instead of using BeautifulSoup, we'll use Playwright with XPaths. I will show you how to come up with the XPaths by inspecting a web page.\n",
    "\n",
    "Let's use Playwright to crawl game forums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e975a67b-df9a-495e-b399-0860da0d49e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "from playwright.async_api import async_playwright"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3964160d-50d9-47bc-a59d-5cd3fa8f6105",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://boardgamegeek.com/boardgame/410201/wyrmspan/forums/66\"\n",
    "base_dir = 'data/raw'\n",
    "delay_seconds = 5\n",
    "\n",
    "if not os.path.exists(base_dir):\n",
    "    os.makedirs(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c687920-b0ee-41e9-bd14-ce98fbf3f132",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_post_path(url):\n",
    "    \"\"\"Return the file path for saving the forum post.\"\"\"\n",
    "    path_components = urlparse(url).path.split('/')\n",
    "    thread, title = path_components[2:4]\n",
    "    return os.path.join(base_dir, f\"{thread}-{title}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "038ed460-2900-44e9-86f6-ff049f8a1c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "playwright = await async_playwright().start()\n",
    "browser = await playwright.chromium.launch(headless=False)\n",
    "page = await browser.new_page()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5ec4d5-1b3f-467b-a9cf-79dcdbbae791",
   "metadata": {},
   "source": [
    "### Get all post links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "138b0ffe-1aa7-4707-93df-5790adaf1d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Playwright uses XPaths to identify elements on the page.\n",
    "# you can find the XPath of an element by right-clicking on the element, \n",
    "# selecting Inspect, then right-clicking on the element in the Elements tab,\n",
    "# selecting Copy, then Copy Full XPath\n",
    "post_xpath = '/html/body/div[2]/main/div[2]/div/div[1]/div[2]/ng-include/div/div/ui-view/ui-view/div/div/div[2]/forums-module/div/div[2]/div[2]/div/forum-threads/ul/li/div[2]/a'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b90bf5bf-9f38-4586-a254-01e3c38e2f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://boardgamegeek.com/boardgame/410201/wyrmspan/forums/66?pageid=1\n",
      "https://boardgamegeek.com/boardgame/410201/wyrmspan/forums/66?pageid=2\n",
      "https://boardgamegeek.com/boardgame/410201/wyrmspan/forums/66?pageid=3\n",
      "https://boardgamegeek.com/boardgame/410201/wyrmspan/forums/66?pageid=4\n",
      "132 132\n"
     ]
    }
   ],
   "source": [
    "post_links = []\n",
    "page_id = 1\n",
    "page_url = page.url\n",
    "while True:\n",
    "    page_url = f'{url}?pageid={page_id}'\n",
    "    print(page_url)\n",
    "    await page.goto(page_url)\n",
    "    await page.wait_for_load_state()\n",
    "    time.sleep(delay_seconds)\n",
    "    if page.url != page_url:\n",
    "        break\n",
    "    for elm in await page.locator(\"xpath=\"+post_xpath).element_handles():\n",
    "        post_url = urljoin(page_url, await elm.get_attribute(\"href\"))\n",
    "        post_links.append(post_url)\n",
    "    page_id += 1\n",
    "        \n",
    "print(len(post_links), len(set(post_links)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8825e1fb-727c-4aae-9dec-74250e941982",
   "metadata": {},
   "source": [
    "### Get the html for each post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9893cc6-ea8e-4c23-92a6-59eb65e4dac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 https://boardgamegeek.com/thread/3328660/rules-card-117\n",
      "352084\n",
      "1 https://boardgamegeek.com/thread/3326902/if-in-later-rounds-i-still-have-coins-but-no-color\n",
      "430734\n",
      "2 https://boardgamegeek.com/thread/3324470/pay-with-cache-resources\n",
      "478858\n",
      "3 https://boardgamegeek.com/thread/3323492/caching-cards-scoring\n",
      "355209\n",
      "4 https://boardgamegeek.com/thread/3322869/play-cave-and-dragon-cards-only-from-hand\n",
      "350544\n",
      "5 https://boardgamegeek.com/thread/3322734/when-is-a-cave-considered-full\n",
      "356373\n",
      "6 https://boardgamegeek.com/thread/3320451/placing-dragon-on-another-dragon\n",
      "354486\n",
      "7 https://boardgamegeek.com/thread/3319935/vp-for-orthogonally-adjacent-dragons\n",
      "352058\n",
      "8 https://boardgamegeek.com/thread/3319882/two-different-benefits\n",
      "353048\n",
      "9 https://boardgamegeek.com/thread/3319383/question-about-having-9-coins-at-the-end-of-your-t\n",
      "476179\n",
      "10 https://boardgamegeek.com/thread/3311620/dragon-71-multicolor-flyer-how-does-it-score-point\n",
      "447284\n",
      "11 https://boardgamegeek.com/thread/3310522/another-timing-question-ancient-one-plus-rainfores\n",
      "347507\n"
     ]
    }
   ],
   "source": [
    "for ix, post_link in enumerate(post_links):\n",
    "    print(ix, post_link)\n",
    "    path = get_post_path(post_link)\n",
    "    await page.goto(post_link)\n",
    "    await page.wait_for_load_state()\n",
    "    time.sleep(delay_seconds)\n",
    "    html = await page.content()\n",
    "    print(len(html))\n",
    "    save_page(path, post_link, html)\n",
    "    if ix > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fdfd7752-5dba-4ed2-a6ca-136ffdb9080e",
   "metadata": {},
   "outputs": [],
   "source": [
    "await browser.close()\n",
    "await playwright.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8aa9bc-a689-4c1d-83bf-f3bd098cef5f",
   "metadata": {},
   "source": [
    "### Extracting data from web pages using BeautifulSoup\n",
    "\n",
    "We'll use BeautifulSoup to extract data from a conference talk and use markdownify to convert HTML to markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "319bb2b7-2ed8-463b-a064-904440f779d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Any, cast\n",
    "\n",
    "from bs4 import BeautifulSoup, Tag\n",
    "from markdownify import MarkdownConverter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a308947-8e8d-425d-ab11-3791afb1a2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/raw/2024-04-13holland.json'\n",
    "bs_parser = 'html.parser'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0006eb8a-b6d3-4d75-a9e9-b0c111208556",
   "metadata": {},
   "outputs": [],
   "source": [
    " def clean(text: Any) -> str:\n",
    "    \"\"\"Convert text to a string and clean it.\"\"\"\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    if isinstance(text, Tag):\n",
    "        text = text.get_text()\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    \"\"\"Replace non-breaking space with normal space and remove surrounding whitespace.\"\"\"\n",
    "    text = text.replace(\" \", \" \").replace(\"\\u200b\", \"\").replace(\"\\u200a\", \" \")\n",
    "    text = re.sub(r\"(\\n\\s*)+\\n\", \"\\n\\n\", text)\n",
    "    text = re.sub(r\" +\\n\", \"\\n\", text)\n",
    "    return cast(str, text.strip())\n",
    "    \n",
    "class ConferenceMarkdownConverter(MarkdownConverter):  # type: ignore\n",
    "    \"\"\"Create a custom MarkdownConverter.\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs: Any):\n",
    "        \"\"\"Initialize custom MarkdownConverter.\"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.base_url = kwargs.get(\"base_url\", \"\")\n",
    "\n",
    "    def convert_a(self, el, text, convert_as_inline):  # type: ignore\n",
    "        \"\"\"Join hrefs with a base url.\"\"\"\n",
    "        if \"href\" in el.attrs:\n",
    "            el[\"href\"] = urljoin(self.base_url, el[\"href\"])\n",
    "        return super().convert_a(el, text, convert_as_inline)\n",
    "\n",
    "    def convert_p(self, el, text, convert_as_inline):  # type: ignore\n",
    "        \"\"\"Add anchor tags to paragraphs with ids.\"\"\"\n",
    "        if el.has_attr(\"id\") and len(el[\"id\"]) > 0:\n",
    "            _id = el[\"id\"]\n",
    "            text = f'<a name=\"{_id}\"></a>{text}'  # noqa: B907\n",
    "        return super().convert_p(el, text, convert_as_inline)\n",
    "\n",
    "# Create shorthand method for custom conversion\n",
    "def _to_markdown(html: str, **options: Any) -> str:\n",
    "    \"\"\"Convert html to markdown.\"\"\"\n",
    "    return cast(str, ConferenceMarkdownConverter(**options).convert(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8c21b5f-6e41-4d9d-ac4c-9eb75014d17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.churchofjesuschrist.org/study/general-conference/2024/04/13holland?lang=eng 198757\n"
     ]
    }
   ],
   "source": [
    "# read conference talk file\n",
    "with open(path, encoding=\"utf8\") as f:\n",
    "    data = json.load(f)\n",
    "print(data['url'], len(data['html']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c979cf73-aca1-4823-a267-033e091ec060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Motions of a Hidden Fire\n",
      "Author: By President Jeffrey R. Holland\n",
      "Author role: Acting President of the Quorum of the Twelve Apostles\n",
      "\n",
      "<a name=\"p4\"></a>Brothers and sisters, I have learned a painful lesson since I last occupied this pulpit in October of 2022. That lesson is: if you don’t give an acceptable talk, you can be banned for the next several conferences. You can see I am assigned early in the first session of this one. What you can’t see is that I am positioned on a trapdoor with a very delicate latch. If this talk doesn’t go well, I won’t see you for another few conferences.\n",
      "\n",
      "<a name=\"p5\"></a>In the spirit of that beautiful hymn with this beautiful choir, I *have* learned some lessons recently that, with the Lord’s help, I wish to share with you today. That will make this a very personal talk.\n",
      "\n",
      "<a name=\"p6\"></a>The most personal and painful of all these recent experiences has been the passing of my beloved wife, Pat. She *was* the greatest woman I have ever known—a perfect wife and mother, to say nothing of her purity, her gift of expression, her spirituality. She gave a talk once titled “Fulfilling the Measure of Your Creation.” I\n"
     ]
    }
   ],
   "source": [
    "url = data['url']\n",
    "html = data['html']\n",
    "soup = BeautifulSoup(html, bs_parser)\n",
    "title = clean(soup.select_one(\"article header h1\").get_text())\n",
    "author = clean(soup.select_one(\"article p.author-name\").get_text())\n",
    "author_role = clean(soup.select_one(\"article p.author-role\").get_text())\n",
    "body = soup.select_one(\"article div.body-block\")\n",
    "markdown = clean(_to_markdown(str(body), base_url=url, heading_style=\"ATX\", strip=[\"script\", \"style\"]))\n",
    "\n",
    "print('Title:', title)\n",
    "print('Author:', author)\n",
    "print('Author role:', author_role)\n",
    "print()\n",
    "print(markdown[:1024])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0bd532-2056-4573-963e-407f69323da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
