{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94697a2f-38c9-4b39-b084-8d3bc9cfc8b2",
   "metadata": {},
   "source": [
    "# Getting Started with Llamaindex\n",
    "\n",
    "Today we are going to create a simple RAG (retrieval-augmented-generation) question-answering chatbot and deploy it.\n",
    "\n",
    "This represents a second way to easily build and deploy RAG projects. We covered the first way in week 8 - using RAGatouille, Gradio, and Huggingface spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957a9748-7995-4ccb-9f70-6fc22be5c404",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "- Make sure you have a recent version of node (I'm using node v18)\n",
    "- Make sure you are using python 3.11\n",
    "  - use mise (or asdf or pyenv) to set the python version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50ea7db-6ed3-4da3-bf64-9d2306d04889",
   "metadata": {},
   "source": [
    "## Create a Llamaindex project\n",
    "- `npx create-llama@latest`\n",
    "  - Project Name? test-llama\n",
    "  - Template? Chat\n",
    "  - Framework? FastAPI (Python)\n",
    "  - Generate a NextJS frontend? Yes\n",
    "  - Obervability? No\n",
    "  - OpenAI key?\n",
    "    - you can use mine if you want to take this approach - ask me\n",
    "  - Data source? Example PDF\n",
    "  - Another data source? No\n",
    "  - LlamaParse? No\n",
    "  - Vector database? ChromaDB\n",
    "  - Agent? leave blank and just press enter\n",
    "  - Proceed? Just generate code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a8c056-a543-4bb8-9b0d-c8a9dd4d18c5",
   "metadata": {},
   "source": [
    "## Start the backend\n",
    "- `cd test-llama/backend`\n",
    "- `poetry install`\n",
    "- `poetry shell`\n",
    "- edit .env: uncomment CHROMA_PATH and set it to `chroma`\n",
    "- `poetry run generate`\n",
    "- `python main.py`\n",
    "- read backend/README.md for more information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e41b0b7-9cc2-4887-8e6a-7b16beeb0221",
   "metadata": {},
   "source": [
    "### Do you get an error about dotenv not being found?\n",
    "\n",
    "If you are using mise and you get this error when you run python main.py, let me know. \n",
    "The problem is that mise put its python path in front of poetry's python path in $PATH.\n",
    "\n",
    "I had to uninstall mise and then re-install it, adding the mise shims to my path instead of activating mise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf708f8-c27f-431f-b67f-1ff1d3badb31",
   "metadata": {},
   "source": [
    "## Start the frontend\n",
    "- open a new terminal window\n",
    "- `cd ../frontend`\n",
    "- `npm install`\n",
    "- `npm run dev`\n",
    "- visit http://localhost:3000 and ask \"what is the format of a postcard?\"\n",
    "\n",
    "Congratulations! You just launched your first Llamaindex app. \n",
    "\n",
    "Now let's push it to github and deploy it so you can share it with your mother."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a4315f-a22e-4dce-80b9-bf0fdc81e8aa",
   "metadata": {},
   "source": [
    "## Edit Dockerfile\n",
    "\n",
    "Replace the last line of the Dockerfile in the frontend directory with the following in order to make it work with Render.\n",
    "\n",
    "`CMD [\"npx\", \"-y\", \"serve@latest\", \"out\"]`\n",
    "\n",
    "Docker is amazing. It's a way to package your application along with its external (operating system) dependencies like the exact versions of python, node, and sqlite. This gives you complete control over the environment in which your application runs. It's like having another operating system inside your operating system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2943c9-a886-4760-b3fc-2c98d70c9fb4",
   "metadata": {},
   "source": [
    "## Push to github\n",
    "\n",
    "- Create a new github repo called test-llama. Make it public but don't add a readme.\n",
    "- From the project root directory\n",
    "  - `git add .`\n",
    "  - `git commit -m 'initial commit'`\n",
    "  - `git remote add origin https://github.com/[your user name]/test-llama.git`\n",
    "  - `git push --set-upstream origin main`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6ff76b-e468-4e35-871e-bb8283e7edb1",
   "metadata": {},
   "source": [
    "## Create a free account on Render to deploy your project\n",
    "\n",
    "*Previous to Render, I tried deploying the project to Vercel and Microsoft Azure. Vercel only supports python 3.9 (they say they support 3.12 but it's broken) and the demo project we just created requires python 3.11. Azure kept giving me the error \"The subscription is not allowed to create or update the serverfarm\", which appears to be a common error over the past several months - you're supposed to submit a support ticket to ask them to fix it, but submitting a support ticket requires a paid subscription.*\n",
    "\n",
    "- Create a Render account\n",
    "  - go to https://render.com click Get Started for Free and create an account using your Github credentials."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d411922b-bf70-4bed-9d11-bdf3f5a54e64",
   "metadata": {},
   "source": [
    "## Deploy the backend\n",
    "\n",
    "We will deploy the backend using Docker.\n",
    "\n",
    "- from the dashboard select Create a new Web Service\n",
    "- select Build and Deploy from a Git repository\n",
    "- connect it to your test-llama github repository\n",
    "- name it [your name]-test-llama-backend\n",
    "- set the root directory to backend\n",
    "- set the runtime to Docker\n",
    "- change the instance type to Free\n",
    "- add the following environment variables from .env\n",
    "  - MODEL_PROVIDER=openai\n",
    "  - MODEL=gpt-3.5-turbo\n",
    "  - EMBEDDING_MODEL=text-embedding-3-large\n",
    "  - EMBEDDING_DIM=1024\n",
    "  - OPENAI_API_KEY=your OpenAI key\n",
    "  - TOP_K=3\n",
    "  - CHROMA_PATH=chroma\n",
    "  - FILESERVER_URL_PREFIX=https://[your name]-test-llama-backend/api/files\n",
    "  - APP_HOST=0.0.0.0\n",
    "  - APP_PORT=80\n",
    "  - ENVIRONMENT=prod\n",
    "  - SYSTEM_PROMPT=You are a helpful assistant who helps users with their questions.\n",
    "- click Create Web Service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fd776b-7461-4ad4-b537-1c720d74b871",
   "metadata": {},
   "source": [
    "## Deploy the frontend\n",
    "\n",
    "We will also deploy the frontend using Docker.\n",
    "\n",
    "- from the dashboard create a new web service\n",
    "- select Build and Deploy from a Git repository\n",
    "- connect it to your test-llama github repository\n",
    "- name it [your name]-test-llama-frontend\n",
    "- set the root directory to frontend\n",
    "- set the runtime to Docker\n",
    "- set the instance type to Free\n",
    "- add the following environment variable\n",
    "  - NEXT_PUBLIC_CHAT_API=https://[your name]-test-llama-backend.onrender.com/api/chat\n",
    "- click Create Web Service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c462395c-1f1e-459c-9d11-055302ad15e8",
   "metadata": {},
   "source": [
    "## Test your new service\n",
    "\n",
    "Go to https://[your-name]-test-llama-frontend.onrender.com and give it a try!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edabdce-f9f0-4a1d-86c3-71362fb975ea",
   "metadata": {},
   "source": [
    "## Where to go from here?\n",
    "\n",
    "What if you wanted to search your own files? Take a look at the backend/data directory. It appears that you can simply replace 101.pdf with your own files, then re-run `poetry run generate` to update the Chroma database with your new data.\n",
    "\n",
    "We will learn more about Llamaindex over the next several months."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
