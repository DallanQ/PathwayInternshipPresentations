{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a43a4b1-b424-41d1-8d39-153cc37f3a08",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization\n",
    "\n",
    "This week will use [Optuna](https://optuna.org/), a library to make finding the best hyperparameters easy. \n",
    "\n",
    "We will use it to discover the best approach for chunking documents and indexing the chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T06:51:20.014116631Z",
     "start_time": "2024-06-02T06:51:20.013126125Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f82c275efc6aa93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T06:51:27.966680878Z",
     "start_time": "2024-06-02T06:51:20.554743009Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "import re\n",
    "\n",
    "import chromadb\n",
    "from llama_index.core import Document, VectorStoreIndex, set_global_handler\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core.node_parser import (\n",
    "    SentenceSplitter,\n",
    "    SemanticSplitterNodeParser,\n",
    "    MarkdownNodeParser,\n",
    ")\n",
    "from llama_index.core.vector_stores.types import VectorStoreQueryMode\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.vector_stores.milvus import MilvusVectorStore\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.embeddings.voyageai import VoyageEmbedding\n",
    "import nest_asyncio\n",
    "import optuna\n",
    "import pandas as pd\n",
    "from pymilvus import MilvusClient\n",
    "from qdrant_client import QdrantClient, AsyncQdrantClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab8255e29444203f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T06:51:28.392069312Z",
     "start_time": "2024-06-02T06:51:27.969236719Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# configure\n",
    "filename = 'everdell.md'\n",
    "qa_filename = 'everdell-selected.csv'\n",
    "ngram_size = 2  # use 2 instead of 3 so we don't skip 2-word header chunks\n",
    "f_beta = 3      # weight recall 3 times as important as precision in f-score\n",
    "n_trials = 25   # number of Optuna trials\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecf5e1c-42fd-4b04-9f26-416d9460bc79",
   "metadata": {},
   "source": [
    "## Define a few helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba60543e-f4f2-466f-8bee-7b577e15da89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams_from_text(text, ngram_size=3):\n",
    "    \"\"\"\n",
    "    Generate ngrams from a specified text string.\n",
    "    \n",
    "    An ngram is a sequence of n words in a row. \n",
    "    For example, if ngram_size=3 and the text was \"You can not play the ranger.\",\n",
    "    this would result in the following list of ngrams: \n",
    "    (you, can, not), (can, not, play), (not, play, the), (play, the ranger).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Lowercase and replace non-alphanumeric characters with spaces\n",
    "    cleaned_text = re.sub(r'[^a-z0-9\\s]', ' ', text.lower())\n",
    "    \n",
    "    # Split text into words\n",
    "    words = cleaned_text.split()\n",
    "    \n",
    "    # Generate ngrams\n",
    "    return [tuple(words[i:i+ngram_size]) for i in range(len(words)+1-ngram_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca2bd89d-7683-4c9c-8bc6-15c91d740db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams_from_texts(texts, ngram_size=3):\n",
    "    \"\"\"Generate all ngrams from a list of texts.\"\"\"\n",
    "    \n",
    "    all_ngrams = []\n",
    "    for text in texts:\n",
    "        ngrams = generate_ngrams_from_text(text, ngram_size=ngram_size)\n",
    "        all_ngrams.extend(ngrams)\n",
    "        \n",
    "    return all_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17ad09f7-3347-4430-b562-f6625ba8cd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall(predicted_ngrams, true_ngrams):\n",
    "    \"\"\"\n",
    "    Return the precision and recall of a predicted list of ngrams by comparing \n",
    "    the predicted ngrams to the true ngrams and calculating the precision and recall.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert lists to sets for easier comparison\n",
    "    predicted_set = set(predicted_ngrams)\n",
    "    true_set = set(true_ngrams)\n",
    "    \n",
    "    # Calculate true positives, false positives, and false negatives\n",
    "    true_positives = len(predicted_set & true_set)\n",
    "    false_positives = len(predicted_set - true_set)\n",
    "    false_negatives = len(true_set - predicted_set)\n",
    "    \n",
    "    # Calculate precision and recall\n",
    "    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n",
    "    \n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcc2b350-d9b9-4d5f-a9c4-b8e6df87bf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_score(precision, recall, beta=1.0):\n",
    "    \"\"\"\n",
    "    Calculate the F-score (harmonic mean) of a given precision and recall, \n",
    "    with an option to weight recall higher.\n",
    "    \n",
    "    We need to calculate an F-score so we can convert the precision and recall metrics \n",
    "    into a single metric so we can say when one (precision, recall) pair is better \n",
    "    than another (precision, recall) pair. Setting the beta greater than 1 allows us to\n",
    "    give more weight to recall than precision in the F-score.\n",
    "    \n",
    "    Parameters:\n",
    "    precision (float): Precision of the model\n",
    "    recall (float): Recall of the model\n",
    "    beta (float): Weight of recall in the harmonic mean (default is 1.0, which means F1 score)\n",
    "    \n",
    "    Returns:\n",
    "    float: The F(beta) score\n",
    "    \"\"\"\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    beta_squared = beta ** 2\n",
    "    return (1 + beta_squared) * (precision * recall) / (beta_squared * precision + recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7da5396-c1fb-493c-a47d-61739303a222",
   "metadata": {},
   "source": [
    "## Read question-answers and generate ngrams from manual quotes\n",
    "\n",
    "The question-answers file has been augmented by a human to include the sentences/paragraphs from the manual that are needed (necessary and sufficient) to answer each question.\n",
    "\n",
    "To evaluate the quality of a list of chunks retrieved from an index, we want to compare the sentences/paragraphs in the chunks against the sentences/paragraphs specified by the human in the question-answer file.\n",
    "\n",
    "To do the comparison we can't simply check for equality, because the retrieved chunk may only overlap part of the human-specified sentence/paragraph. So we generate *ngrams* for the retrieved chunks and the human-specified sentence/paragraphs, and compare how many ngrams they have in common using the standard precision and recall metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4507f257-7671-4fdc-a069-187289b61f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>manual quote 1</th>\n",
       "      <th>manual quote 2</th>\n",
       "      <th>manual quote 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://boardgamegeek.com/thread/2440267/first-couple-playthroughs-gathered-questions</td>\n",
       "      <td>What is the die for?</td>\n",
       "      <td>It’s for the solo game.</td>\n",
       "      <td>solo rules</td>\n",
       "      <td>To play Rugwort's card, roll the 8-sided die</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://boardgamegeek.com/thread/2017107/new-question-about-dungeon-card</td>\n",
       "      <td>Let's say I have 15 cards in my city, and I have the Dungeon and someone already in the one cell. Can I play the Ranger to unlock the second cell, putting a different Critter already in my city into the now-unlocked second cell?</td>\n",
       "      <td>You could not play the Ranger as it would get you past the city's limit. However if you have Dungeon with one prisoner and Ranger in your city and you are at 15 played slots, you could use the Ranger's power to get an existing Critter into the 2nd cell.</td>\n",
       "      <td>\"Your city has a maximum of 15 spaces \\nto play cards into. Each card takes up one \\nspace. Recommended layout is 3 rows with \\n5 cards in each. Event cards do not count \\nagainst this 15 card limit.\"</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://boardgamegeek.com/thread/2017107/new-question-about-dungeon-card</td>\n",
       "      <td>Can I play a NEW card when all 15 seats in the city are occupied, but there is a free cell in the Dungeon?</td>\n",
       "      <td>Yes, this could be done.</td>\n",
       "      <td>\"Your city has a maximum of 15 spaces \\nto play cards into. Each card takes up one \\nspace. Recommended layout is 3 rows with \\n5 cards in each. Event cards do not count \\nagainst this 15 card limit.\"</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                     url  \\\n",
       "0  https://boardgamegeek.com/thread/2440267/first-couple-playthroughs-gathered-questions   \n",
       "1               https://boardgamegeek.com/thread/2017107/new-question-about-dungeon-card   \n",
       "2               https://boardgamegeek.com/thread/2017107/new-question-about-dungeon-card   \n",
       "\n",
       "                                                                                                                                                                                                                               question  \\\n",
       "0                                                                                                                                                                                                                  What is the die for?   \n",
       "1  Let's say I have 15 cards in my city, and I have the Dungeon and someone already in the one cell. Can I play the Ranger to unlock the second cell, putting a different Critter already in my city into the now-unlocked second cell?   \n",
       "2                                                                                                                            Can I play a NEW card when all 15 seats in the city are occupied, but there is a free cell in the Dungeon?   \n",
       "\n",
       "                                                                                                                                                                                                                                                          answer  \\\n",
       "0                                                                                                                                                                                                                                        It’s for the solo game.   \n",
       "1  You could not play the Ranger as it would get you past the city's limit. However if you have Dungeon with one prisoner and Ranger in your city and you are at 15 played slots, you could use the Ranger's power to get an existing Critter into the 2nd cell.   \n",
       "2                                                                                                                                                                                                                                      Yes, this could be done.    \n",
       "\n",
       "                                                                                                                                                                                             manual quote 1  \\\n",
       "0                                                                                                                                                                                                solo rules   \n",
       "1  \"Your city has a maximum of 15 spaces \\nto play cards into. Each card takes up one \\nspace. Recommended layout is 3 rows with \\n5 cards in each. Event cards do not count \\nagainst this 15 card limit.\"   \n",
       "2  \"Your city has a maximum of 15 spaces \\nto play cards into. Each card takes up one \\nspace. Recommended layout is 3 rows with \\n5 cards in each. Event cards do not count \\nagainst this 15 card limit.\"   \n",
       "\n",
       "                                 manual quote 2 manual quote 3  \n",
       "0  To play Rugwort's card, roll the 8-sided die                 \n",
       "1                                                               \n",
       "2                                                               "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read question-answers\n",
    "qa_df = pd.read_csv(f'data/{qa_filename}', na_filter=False)\n",
    "print(len(qa_df))\n",
    "qa_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a4e00f0-db84-46c5-8878-b3247a197b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "# NOTE: we shouldn't include questions in the *test* set right now,\n",
    "# but people are still adding the manual quotes, \n",
    "# and since we have so few questions with manual quotes so far \n",
    "# we will use all of them for this demo.\n",
    "\n",
    "# keep only rows with at least 1 manual quote\n",
    "qa_df = qa_df[qa_df['manual quote 1'].notna() & (qa_df['manual quote 1'] != '')]\n",
    "print(len(qa_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0014b748-be9d-4464-abb1-a1a95bedf0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "# generate bigrams (ngram size=2) for each manual quote\n",
    "# and store them in the question_ngrams dictionary\n",
    "question_ngrams = {}\n",
    "for _, row in qa_df.iterrows():\n",
    "    question = row['question']\n",
    "    all_ngrams = []\n",
    "    for column in qa_df.columns:\n",
    "        if column in ['url', 'question', 'answer']:\n",
    "            continue\n",
    "        text = row[column]\n",
    "        if not text:\n",
    "            continue\n",
    "        all_ngrams.extend(generate_ngrams_from_text(text, ngram_size=ngram_size))\n",
    "    if len(all_ngrams) == 0:\n",
    "        print('ERROR: no ngrams in quotes for ', question)\n",
    "        continue\n",
    "    question_ngrams[question] = all_ngrams\n",
    "print(len(question_ngrams))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a50357-185e-44ce-8a46-de103f83b9f4",
   "metadata": {},
   "source": [
    "## Read the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fa9d9ac-b0ac-4ed9-9430-e864e0f5df79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21237\n"
     ]
    }
   ],
   "source": [
    "# load document\n",
    "documents = []\n",
    "with open(f'data/{filename}', 'r', encoding='utf-8') as file:\n",
    "    document = Document(\n",
    "        text = file.read(),\n",
    "        metadata = {\"filename\": filename},\n",
    "    )\n",
    "    # add the document to a single-entry documents list that we will use below\n",
    "    documents.append(document)\n",
    "print(len(documents[0].text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa15844d-4cc3-4fbd-9c97-2352caed3a25",
   "metadata": {},
   "source": [
    "## Optimize hyperparameters by creating an index and evaluating the retrieved chunks\n",
    "\n",
    "Creating an index involves a sequence of steps (a pipeline). Each step is configured using hyperparameters:\n",
    "- split each document into chunks\n",
    "- add metadata - e.g., document title, summary of previous and next chunks, pointer to parent chunk\n",
    "- add an embedding (vector) - decide whether you want the embedding to include chunk metadata or just the text\n",
    "- index the chunk - choose a vector store and index the embeddings, keywords, or both\n",
    "\n",
    "Evaluate the retrieved chunks\n",
    "- issue the queries\n",
    "- compare the ngrams in the retrieved chunks to the ngrams in the human-specified sentences/paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7188773706598f27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T06:51:38.604999793Z",
     "start_time": "2024-06-02T06:51:34.534023729Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    This function is called by Optuna. It creates an index, run queries over the index, \n",
    "    calculates the precision and recall of the results, and returns the average f-score.\n",
    "    \"\"\"\n",
    "\n",
    "    #\n",
    "    # Define hyperparameters\n",
    "    # \n",
    "    \n",
    "    # define embedder\n",
    "    embed_model_name = trial.suggest_categorical('embed_model', [\n",
    "        'text-embedding-3-small', \n",
    "        'voyage-large-2-instruct',\n",
    "        # GPU runs out of memory with gte model\n",
    "        # 'Alibaba-NLP/gte-large-en-v1.5',  # WARNING: this downloads 1.74G of data\n",
    "    ])\n",
    "    if embed_model_name == 'text-embedding-3-small':\n",
    "        embed_model = OpenAIEmbedding(\n",
    "            model=embed_model_name,\n",
    "            embed_batch_size=10,\n",
    "            max_retries=25,\n",
    "            timeout=180,\n",
    "            reuse_client=False,\n",
    "        )\n",
    "    elif embed_model_name == 'voyage-large-2-instruct':\n",
    "        embed_model = VoyageEmbedding(\n",
    "            model_name=embed_model_name,\n",
    "        )\n",
    "    elif embed_model_name == 'Alibaba-NLP/gte-large-en-v1.5':\n",
    "        embed_model = HuggingFaceEmbedding(model_name=embed_model_name, trust_remote_code=True)\n",
    "        \n",
    "    # define splitter\n",
    "    splitter_name = trial.suggest_categorical('splitter', [\n",
    "        'sentence', \n",
    "        'semantic',\n",
    "        'markdown',\n",
    "    ])\n",
    "    if splitter_name == 'sentence':\n",
    "        chunk_size = trial.suggest_int('chunk_size', 256, 1024)\n",
    "        chunk_overlap = trial.suggest_int('chunk_overlap', 0, 200)\n",
    "        splitter = SentenceSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    elif splitter_name == 'semantic':\n",
    "        buffer_size = trial.suggest_int('buffer_size', 1, 3)\n",
    "        breakpoint_percentile_threshold = trial.suggest_int('breakpoint_percentile_threshold', 60, 95)\n",
    "        include_prev_next_rel = trial.suggest_categorical('include_prev_next_rel', [True, False])\n",
    "        splitter = SemanticSplitterNodeParser(\n",
    "            buffer_size=buffer_size, \n",
    "            breakpoint_percentile_threshold=breakpoint_percentile_threshold, \n",
    "            include_prev_next_rel=include_prev_next_rel,\n",
    "            embed_model=embed_model,\n",
    "        )\n",
    "    elif splitter_name == 'markdown':\n",
    "        include_prev_next_rel = trial.suggest_categorical('include_prev_next_rel', [True, False])\n",
    "        splitter = MarkdownNodeParser(\n",
    "            include_prev_next_rel=include_prev_next_rel,\n",
    "        )\n",
    "\n",
    "    # add metadata\n",
    "    ## nothing for now\n",
    "\n",
    "    # define index\n",
    "    query_mode = VectorStoreQueryMode.DEFAULT\n",
    "    index_type = trial.suggest_categorical('index', [\n",
    "        'chromadb',\n",
    "        'qdrant',\n",
    "        # Milvus cloud doesn't support hybrid indices yet\n",
    "        # 'milvus',  # need to create a (free) account at https://cloud.zilliz.com/ \n",
    "                   # and add MILVUS_URI=your public endpoint and MILVUS_TOKEN=your token (api key) to your .env file\n",
    "    ])\n",
    "    if index_type == 'chromadb':\n",
    "        chroma_client = chromadb.EphemeralClient()\n",
    "        # delete collection if it exists\n",
    "        if any(coll.name == 'test' for coll in chroma_client.list_collections()):\n",
    "            chroma_client.delete_collection('test')        \n",
    "        chroma_collection = chroma_client.create_collection('test')\n",
    "        vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "    elif index_type == 'qdrant':\n",
    "        query_mode = VectorStoreQueryMode.HYBRID\n",
    "        client = QdrantClient(location=':memory:')\n",
    "        # delete collection if it exists\n",
    "        if client.collection_exists('test'):\n",
    "            client.delete_collection('test')        \n",
    "        # create our vector store with hybrid indexing enabled\n",
    "        # batch_size controls how many nodes are encoded with sparse vectors at once\n",
    "        # hybrid uses Splade v1 for sparse vectors\n",
    "        vector_store = QdrantVectorStore(\n",
    "            collection_name='test',\n",
    "            client=client,\n",
    "            enable_hybrid=True,\n",
    "            batch_size=20,\n",
    "        )\n",
    "    elif index_type == 'milvus':\n",
    "        query_mode = VectorStoreQueryMode.HYBRID\n",
    "        milvus_k = trial.suggest_int('milvus_k', 40, 80)\n",
    "        # delete collection if it exists\n",
    "        client = MilvusClient(uri=os.environ['MILVUS_URI'], token=os.environ['MILVUS_TOKEN'])\n",
    "        if client.has_collection('test'):\n",
    "            client.drop_collection(collection_name='test')\n",
    "        client.close()\n",
    "        # hybrid uses BGE-M3 for sparse vectors\n",
    "        vector_store = MilvusVectorStore(\n",
    "            uri=os.environ['MILVUS_URI'], \n",
    "            token=os.environ['MILVUS_TOKEN'],\n",
    "            collection_name='test',\n",
    "            dim=len(embed_model.get_text_embedding('foo')),\n",
    "            overwrite=True,\n",
    "            enable_sparse=True,\n",
    "            hybrid_ranker='RRFRanker',\n",
    "            hybrid_ranker_params={'k': milvus_k},\n",
    "        )\n",
    "\n",
    "    # define top_k\n",
    "    top_k = trial.suggest_int('top_k', 2, 5)\n",
    "    sparse_top_k = top_k * 5\n",
    "\n",
    "    # \n",
    "    # Use hyperparameters to split documents into chunks, generate embeddings, and insert into an index\n",
    "    #\n",
    "    \n",
    "    # create a simple ingestion pipeline: chunk the documents and create embeddings\n",
    "    pipeline = IngestionPipeline(transformations=[\n",
    "        splitter,\n",
    "        embed_model,\n",
    "    ])\n",
    "\n",
    "    # create an index from the vector store\n",
    "    index = VectorStoreIndex.from_vector_store(\n",
    "        vector_store,\n",
    "        embed_model=embed_model,\n",
    "    )\n",
    "\n",
    "    # run the pipeline to generate nodes\n",
    "    nodes = pipeline.run(documents=documents)\n",
    "    # print('nodes', len(nodes))\n",
    "\n",
    "    # add the nodes to the index\n",
    "    index.insert_nodes(nodes)\n",
    "\n",
    "    # assert all nodes have been indexed\n",
    "    assert len(index.as_retriever(similarity_top_k=len(nodes)).retrieve('foo')) == len(nodes)\n",
    "\n",
    "    # create a retriever from the index\n",
    "    retriever = index.as_retriever(\n",
    "        vector_store_query_mode=query_mode,\n",
    "        similarity_top_k=top_k,\n",
    "        sparse_top_k=sparse_top_k,\n",
    "    )\n",
    "\n",
    "    #\n",
    "    # Evaluate the quality of the chunks retrieved from the index for the sample questions\n",
    "    #\n",
    "    \n",
    "    # issue all questions and calculate the f-score on the retrieved chunks\n",
    "    f_scores = []\n",
    "    for question, true_ngrams in question_ngrams.items():\n",
    "        response = retriever.retrieve(question)\n",
    "        # print([node.id_ for node in response])\n",
    "        predicted_ngrams = generate_ngrams_from_texts([node.text for node in response], ngram_size=ngram_size)\n",
    "        precision, recall = precision_recall(predicted_ngrams, true_ngrams)\n",
    "        score = f_score(precision, recall, beta=f_beta)\n",
    "        f_scores.append(score)\n",
    "    avg_f_score = sum(f_scores) / len(f_scores)\n",
    "\n",
    "    # return the average f-score\n",
    "    return avg_f_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ed2aacf-8368-4796-9998-b209c24a3569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To see a dashboard, open a terminal, activate the virtual environment, and run: optuna-dashboard sqlite:///optuna-test.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-23 18:19:30,385] A new study created in RDB with name: test\n",
      "[I 2024-06-23 18:20:30,617] Trial 0 finished with value: 0.21598551827159682 and parameters: {'embed_model': 'text-embedding-3-small', 'splitter': 'markdown', 'include_prev_next_rel': True, 'index': 'chromadb', 'top_k': 4}. Best is trial 0 with value: 0.21598551827159682.\n",
      "[I 2024-06-23 18:21:27,156] Trial 1 finished with value: 0.25620708209243037 and parameters: {'embed_model': 'voyage-large-2-instruct', 'splitter': 'semantic', 'buffer_size': 2, 'breakpoint_percentile_threshold': 80, 'include_prev_next_rel': True, 'index': 'chromadb', 'top_k': 3}. Best is trial 1 with value: 0.25620708209243037.\n",
      "[I 2024-06-23 18:22:00,417] Trial 2 finished with value: 0.24174347524532452 and parameters: {'embed_model': 'voyage-large-2-instruct', 'splitter': 'markdown', 'include_prev_next_rel': False, 'index': 'chromadb', 'top_k': 2}. Best is trial 1 with value: 0.25620708209243037.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "709c87b3c1ea4b5fa776f95e1017152d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dd5cb67a71d4d59a353a2c716bf725b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNI [root] Payload indexes have no effect in the local Qdrant. Please use server Qdrant if you need payload indexes.\n",
      "[I 2024-06-23 18:22:42,762] Trial 3 finished with value: 0.2548235158629432 and parameters: {'embed_model': 'voyage-large-2-instruct', 'splitter': 'sentence', 'chunk_size': 285, 'chunk_overlap': 123, 'index': 'qdrant', 'top_k': 4}. Best is trial 1 with value: 0.25620708209243037.\n",
      "[I 2024-06-23 18:25:10,278] Trial 4 finished with value: 0.30502262085574366 and parameters: {'embed_model': 'text-embedding-3-small', 'splitter': 'semantic', 'buffer_size': 2, 'breakpoint_percentile_threshold': 61, 'include_prev_next_rel': False, 'index': 'chromadb', 'top_k': 5}. Best is trial 4 with value: 0.30502262085574366.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f75538e1db24177a7e13392d45cd875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3c5d971774b47d69a3d22b8562f3cb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNI [root] Payload indexes have no effect in the local Qdrant. Please use server Qdrant if you need payload indexes.\n",
      "[I 2024-06-23 18:26:14,438] Trial 5 finished with value: 0.2261776199930114 and parameters: {'embed_model': 'voyage-large-2-instruct', 'splitter': 'markdown', 'include_prev_next_rel': False, 'index': 'qdrant', 'top_k': 2}. Best is trial 4 with value: 0.30502262085574366.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "284d35234a594e7ebab3683c0074cc70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d801db34efc40caac5a259168231e80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNI [root] Payload indexes have no effect in the local Qdrant. Please use server Qdrant if you need payload indexes.\n",
      "[I 2024-06-23 18:27:14,207] Trial 6 finished with value: 0.2261776199930114 and parameters: {'embed_model': 'voyage-large-2-instruct', 'splitter': 'markdown', 'include_prev_next_rel': True, 'index': 'qdrant', 'top_k': 2}. Best is trial 4 with value: 0.30502262085574366.\n",
      "[I 2024-06-23 18:27:48,620] Trial 7 finished with value: 0.17283256284527276 and parameters: {'embed_model': 'voyage-large-2-instruct', 'splitter': 'sentence', 'chunk_size': 721, 'chunk_overlap': 151, 'index': 'chromadb', 'top_k': 3}. Best is trial 4 with value: 0.30502262085574366.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "739ca63e041c40fab750ca08f3c6578f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2684f8de06e479b82ddf8d339b41e48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNI [root] Payload indexes have no effect in the local Qdrant. Please use server Qdrant if you need payload indexes.\n",
      "[I 2024-06-23 18:28:50,380] Trial 8 finished with value: 0.1476844907886592 and parameters: {'embed_model': 'voyage-large-2-instruct', 'splitter': 'sentence', 'chunk_size': 1022, 'chunk_overlap': 123, 'index': 'qdrant', 'top_k': 3}. Best is trial 4 with value: 0.30502262085574366.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69b9b2f6fbfc4a9f87785315e86202ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6370989708244a7c86ea7924ee644cff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNI [root] Payload indexes have no effect in the local Qdrant. Please use server Qdrant if you need payload indexes.\n",
      "[I 2024-06-23 18:30:11,159] Trial 9 finished with value: 0.20464838672264968 and parameters: {'embed_model': 'voyage-large-2-instruct', 'splitter': 'sentence', 'chunk_size': 393, 'chunk_overlap': 67, 'index': 'qdrant', 'top_k': 5}. Best is trial 4 with value: 0.30502262085574366.\n",
      "[I 2024-06-23 18:33:40,489] Trial 10 finished with value: 0.2845149991991019 and parameters: {'embed_model': 'text-embedding-3-small', 'splitter': 'semantic', 'buffer_size': 2, 'breakpoint_percentile_threshold': 61, 'include_prev_next_rel': False, 'index': 'chromadb', 'top_k': 5}. Best is trial 4 with value: 0.30502262085574366.\n",
      "[I 2024-06-23 18:35:25,866] Trial 11 finished with value: 0.2992974061018135 and parameters: {'embed_model': 'text-embedding-3-small', 'splitter': 'semantic', 'buffer_size': 2, 'breakpoint_percentile_threshold': 62, 'include_prev_next_rel': False, 'index': 'chromadb', 'top_k': 5}. Best is trial 4 with value: 0.30502262085574366.\n",
      "[I 2024-06-23 18:37:21,807] Trial 12 finished with value: 0.3240292971708032 and parameters: {'embed_model': 'text-embedding-3-small', 'splitter': 'semantic', 'buffer_size': 1, 'breakpoint_percentile_threshold': 60, 'include_prev_next_rel': False, 'index': 'chromadb', 'top_k': 5}. Best is trial 12 with value: 0.3240292971708032.\n",
      "[I 2024-06-23 18:39:26,391] Trial 13 finished with value: 0.345060812658343 and parameters: {'embed_model': 'text-embedding-3-small', 'splitter': 'semantic', 'buffer_size': 1, 'breakpoint_percentile_threshold': 60, 'include_prev_next_rel': False, 'index': 'chromadb', 'top_k': 4}. Best is trial 13 with value: 0.345060812658343.\n",
      "[I 2024-06-23 18:41:20,822] Trial 14 finished with value: 0.1901970194384464 and parameters: {'embed_model': 'text-embedding-3-small', 'splitter': 'semantic', 'buffer_size': 1, 'breakpoint_percentile_threshold': 95, 'include_prev_next_rel': False, 'index': 'chromadb', 'top_k': 4}. Best is trial 13 with value: 0.345060812658343.\n",
      "[I 2024-06-23 18:43:31,482] Trial 15 finished with value: 0.2740607715586396 and parameters: {'embed_model': 'text-embedding-3-small', 'splitter': 'semantic', 'buffer_size': 1, 'breakpoint_percentile_threshold': 71, 'include_prev_next_rel': False, 'index': 'chromadb', 'top_k': 4}. Best is trial 13 with value: 0.345060812658343.\n",
      "[I 2024-06-23 18:45:34,117] Trial 16 finished with value: 0.31268595636354246 and parameters: {'embed_model': 'text-embedding-3-small', 'splitter': 'semantic', 'buffer_size': 1, 'breakpoint_percentile_threshold': 71, 'include_prev_next_rel': False, 'index': 'chromadb', 'top_k': 5}. Best is trial 13 with value: 0.345060812658343.\n",
      "[I 2024-06-23 18:47:28,154] Trial 17 finished with value: 0.2600918345212877 and parameters: {'embed_model': 'text-embedding-3-small', 'splitter': 'semantic', 'buffer_size': 3, 'breakpoint_percentile_threshold': 70, 'include_prev_next_rel': False, 'index': 'chromadb', 'top_k': 4}. Best is trial 13 with value: 0.345060812658343.\n",
      "[I 2024-06-23 18:49:13,075] Trial 18 finished with value: 0.24619916123389426 and parameters: {'embed_model': 'text-embedding-3-small', 'splitter': 'semantic', 'buffer_size': 1, 'breakpoint_percentile_threshold': 83, 'include_prev_next_rel': True, 'index': 'chromadb', 'top_k': 5}. Best is trial 13 with value: 0.345060812658343.\n",
      "[I 2024-06-23 18:51:18,968] Trial 19 finished with value: 0.3384383398673504 and parameters: {'embed_model': 'text-embedding-3-small', 'splitter': 'semantic', 'buffer_size': 1, 'breakpoint_percentile_threshold': 67, 'include_prev_next_rel': False, 'index': 'chromadb', 'top_k': 4}. Best is trial 13 with value: 0.345060812658343.\n",
      "[I 2024-06-23 18:53:20,542] Trial 20 finished with value: 0.2753254950825273 and parameters: {'embed_model': 'text-embedding-3-small', 'splitter': 'semantic', 'buffer_size': 3, 'breakpoint_percentile_threshold': 67, 'include_prev_next_rel': False, 'index': 'chromadb', 'top_k': 3}. Best is trial 13 with value: 0.345060812658343.\n",
      "INFO  [backoff] Backing off send_request(...) for 0.3s (requests.exceptions.ReadTimeout: HTTPSConnectionPool(host='us-api.i.posthog.com', port=443): Read timed out. (read timeout=15))\n",
      "[I 2024-06-23 18:55:56,862] Trial 21 finished with value: 0.32988412640402137 and parameters: {'embed_model': 'text-embedding-3-small', 'splitter': 'semantic', 'buffer_size': 1, 'breakpoint_percentile_threshold': 66, 'include_prev_next_rel': False, 'index': 'chromadb', 'top_k': 4}. Best is trial 13 with value: 0.345060812658343.\n",
      "[I 2024-06-23 18:58:35,495] Trial 22 finished with value: 0.32939100038978003 and parameters: {'embed_model': 'text-embedding-3-small', 'splitter': 'semantic', 'buffer_size': 1, 'breakpoint_percentile_threshold': 66, 'include_prev_next_rel': False, 'index': 'chromadb', 'top_k': 4}. Best is trial 13 with value: 0.345060812658343.\n",
      "[I 2024-06-23 19:01:26,575] Trial 23 finished with value: 0.27273752763317644 and parameters: {'embed_model': 'text-embedding-3-small', 'splitter': 'semantic', 'buffer_size': 1, 'breakpoint_percentile_threshold': 76, 'include_prev_next_rel': False, 'index': 'chromadb', 'top_k': 4}. Best is trial 13 with value: 0.345060812658343.\n",
      "[I 2024-06-23 19:04:18,108] Trial 24 finished with value: 0.32817242999687896 and parameters: {'embed_model': 'text-embedding-3-small', 'splitter': 'semantic', 'buffer_size': 1, 'breakpoint_percentile_threshold': 66, 'include_prev_next_rel': False, 'index': 'chromadb', 'top_k': 4}. Best is trial 13 with value: 0.345060812658343.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'embed_model': 'text-embedding-3-small',\n",
       " 'splitter': 'semantic',\n",
       " 'buffer_size': 1,\n",
       " 'breakpoint_percentile_threshold': 60,\n",
       " 'include_prev_next_rel': False,\n",
       " 'index': 'chromadb',\n",
       " 'top_k': 4}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ask Optuna to find the best hyperparameters\n",
    "\n",
    "study_name = 'test'  # Unique identifier of the study.\n",
    "storage_name = f\"sqlite:///optuna-{study_name}.db\"\n",
    "print(f\"To see a dashboard, open a terminal, activate the virtual environment, and run: optuna-dashboard {storage_name}\")\n",
    "study = optuna.create_study(\n",
    "    study_name=study_name, \n",
    "    storage=storage_name,\n",
    "    load_if_exists=True,\n",
    "    direction='maximize',\n",
    ")\n",
    "study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f8ef63-8ded-440d-875e-3e6216ab503b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
